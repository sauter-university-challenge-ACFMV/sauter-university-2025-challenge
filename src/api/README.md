# üìò Documenta√ß√£o API de Ingest√£o de Dados ONS

## 1. Vis√£o Geral e Arquitetura

Esta API foi projetada para atuar como um servi√ßo de ingest√£o de dados, buscando, processando e armazenando de forma padronizada os datasets p√∫blicos do Operador Nacional do Sistema El√©trico (ONS). A arquitetura √© modular e segue o princ√≠pio de separa√ß√£o de responsabilidades, dividida nas seguintes camadas:

* **Roteamento (Routers)**: Respons√°vel por expor os endpoints da API, receber as requisi√ß√µes e invocar a camada de servi√ßo. Utiliza o FastAPI.
* **Servi√ßo (Services)**: Cont√©m a l√≥gica de neg√≥cio principal, orquestrando o fluxo de busca, processamento e armazenamento dos dados.
* **Reposit√≥rio (Repositories)**: Abstrai o acesso a fontes de dados externas, como o Google Cloud Storage (GCS) e o BigQuery.

O projeto √© conteinerizado com Docker, utiliza o `uv` para gerenciamento de pacotes e √© configurado via vari√°veis de ambiente.

## 2. Fluxo de Dados Detalhado

O processo de ingest√£o, do in√≠cio ao fim, segue os passos abaixo:

1.  **Requisi√ß√£o**: O cliente envia uma requisi√ß√£o `POST` para um dos endpoints (`/filter-parquet-files` ou `/bulk-ingest-parquet-files`) com um DTO contendo os filtros (ano, pacote, etc.).
2.  **Roteamento**: O `OnsRouter` recebe a requisi√ß√£o, valida o corpo com o Pydantic DTO (`DateFilterDTO`) e chama o m√©todo correspondente no `OnsService`.
3.  **Busca de Metadados (Service)**: O `OnsService` constr√≥i a URL da API da ONS e busca os metadados do pacote solicitado para identificar os recursos (arquivos) dispon√≠veis.
4.  **Filtragem e Sele√ß√£o (Service)**: Os recursos s√£o filtrados por ano e tipo de arquivo, priorizando `parquet`, `csv` e `xlsx`. A l√≥gica seleciona o melhor formato dispon√≠vel para cada ano dentro do intervalo solicitado.
5.  **Processamento Concorrente (Service)**: Para cada recurso selecionado, uma tarefa de download e processamento √© criada e executada de forma concorrente usando `asyncio.gather`.
6.  **Download e Convers√£o (Service)**:
    * O conte√∫do do arquivo √© baixado em mem√≥ria.
    * O `pandas` √© utilizado para ler os dados (seja CSV, XLSX ou Parquet) e converter todas as colunas para o tipo `string`, garantindo a consist√™ncia do schema.
    * O DataFrame resultante √© serializado para um buffer em formato Parquet.
7.  **Verifica√ß√£o de Duplicidade (Repository)**:
    * O `OnsService` extrai a data mais recente do DataFrame.
    * Ele ent√£o invoca o m√©todo `raw_table_has_value` do `GCSFileRepository`. Este m√©todo verifica no BigQuery se um registro com essa data j√° existe na tabela de destino, evitando o reprocessamento de dados.
8.  **Upload para GCS (Repository)**: Se os dados forem in√©ditos, o `OnsService` chama o m√©todo `save` do `GCSFileRepository`, que faz o upload do buffer Parquet para o bucket no GCS. A estrutura do caminho no GCS √© montada de forma a organizar os arquivos por pacote, ano, m√™s e dia.
9.  **Resposta**: O `OnsService` compila os resultados (sucessos e falhas) em um objeto `ProcessResponse` e o retorna ao `OnsRouter`, que formata a resposta HTTP final para o cliente.

## 3. Camada de Servi√ßo (`OnsService`)

A classe `OnsService` (`api/services/ons_service.py`) √© o n√∫cleo da aplica√ß√£o, orquestrando todo o fluxo de ingest√£o.

### Principais M√©todos

* `process_reservoir_data(filters: DateFilterDTO)`:
    * Recebe um √∫nico DTO de filtro.
    * Busca os metadados do pacote na ONS.
    * Filtra os recursos por ano e formato, selecionando a melhor op√ß√£o para cada ano.
    * Cria e executa tarefas de download (`_download_parquet`) de forma concorrente.
    * Agrega os resultados e retorna um `ProcessResponse` com o resumo da opera√ß√£o.

* `process_reservoir_data_bulk(filters_list: List[DateFilterDTO])`:
    * Recebe uma lista de DTOs de filtro.
    * Cria uma tarefa `process_reservoir_data` para cada DTO da lista.
    * Executa todas as tarefas concorrentemente, permitindo a ingest√£o em massa de diferentes pacotes ou per√≠odos.

* `_download_parquet(client: httpx.AsyncClient, download_info: DownloadInfo)`:
    * M√©todo auxiliar que executa o fluxo de um √∫nico arquivo.
    * **Etapas**:
        1.  Baixa o conte√∫do do arquivo da URL (`_fetch_bytes`).
        2.  L√™ os dados para um DataFrame pandas (`_read_to_dataframe`).
        3.  Padroniza todas as colunas para string (`_convert_all_columns_to_string`).
        4.  Converte o DataFrame para um buffer em mem√≥ria no formato Parquet (`_dataframe_to_parquet_buffer`).
        5.  Verifica no BigQuery se o dado j√° existe, consultando a √∫ltima data do arquivo.
        6.  Se o dado for novo, faz o upload para o GCS (`_save_to_gcs`).
    * Retorna um `DownloadResult` detalhando o sucesso ou a falha da opera√ß√£o, incluindo mensagens de erro.

## 4. Camada de Reposit√≥rio (`GCSFileRepository`)

A classe `GCSFileRepository` (`api/repositories/gcs_repository.py`) abstrai toda a intera√ß√£o com os servi√ßos do Google Cloud.

### Autentica√ß√£o

O m√©todo `_create_storage_client` implementa uma cadeia de estrat√©gias de autentica√ß√£o para se conectar ao GCP, na seguinte ordem de prioridade:

1.  **`GOOGLE_CREDENTIALS_JSON`**: Tenta carregar as credenciais a partir de uma string JSON na vari√°vel de ambiente. Ideal para ambientes de CI/CD e deployments conteinerizados (ex: Cloud Run).
2.  **`GOOGLE_APPLICATION_CREDENTIALS`**: Tenta carregar as credenciais a partir do caminho de um arquivo de chave de servi√ßo.
3.  **`GOOGLE_CLOUD_PROJECT`**: Tenta autenticar usando o ID do projeto, contando com as credenciais do ambiente (ex: gcloud CLI local).
4.  **Credenciais Padr√£o**: Como √∫ltima tentativa, utiliza as credenciais padr√£o do ambiente.

Se todos os m√©todos falharem, uma exce√ß√£o √© levantada com uma mensagem clara sobre como configurar a autentica√ß√£o.

### Principais M√©todos

* `save(file: IO[bytes], filename: str, _bucket_name: str | None)`:
    * Recebe um buffer de bytes e um nome de arquivo.
    * Faz o upload do arquivo para o GCS no bucket especificado (ou no bucket padr√£o).
    * Retorna a URL p√∫blica do objeto no GCS.

* `raw_table_has_value(package_name: str, column_name: str, last_day: str)`:
    * Primeiro, verifica se a tabela de destino existe no BigQuery com o m√©todo `_table_exists`.
    * Se a tabela existir, executa uma query `SELECT 1 ... WHERE <coluna_data> = @last_day LIMIT 1`.
    * Retorna `True` se a query retornar alguma linha (o dado j√° existe) e `False` caso contr√°rio. Isso √© crucial para garantir a idempot√™ncia do processo de ingest√£o.

## 5. Modelos de Dados (DTOs)

Os modelos em `api/models/ons_dto.py` usam Pydantic para definir as estruturas de dados e garantir a valida√ß√£o autom√°tica.

* **`DateFilterDTO`**: Define o contrato de entrada para os endpoints. Garante que os tipos de dados estejam corretos e permite que campos sejam opcionais.
* **`DownloadInfo`**, **`DownloadResult`**, **`ProcessResponse`**: Modelos internos usados na camada de servi√ßo para estruturar os dados durante o fluxo de processamento, garantindo clareza e consist√™ncia entre os m√©todos.
* **`ApiResponse`**, **`ApiBulkResponse`**: Modelam a estrutura final das respostas JSON enviadas aos clientes, definindo um contrato claro de sa√≠da.